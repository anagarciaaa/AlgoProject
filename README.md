# COP4533 - Algorithm Abstraction & Design Programming Project
Milestone 1: 
Ana - Currently Working on Algorithm 1 Problem S1 
---

## üß© Problem S1 ‚Äì Monotonically Non-Decreasing Values

### üí° Greedy Idea

Since the vault values always go up or stay the same (they never go down), the vaults on the right side will always be as good or better than the ones on the left.  
That means it makes the most sense to **start picking from the end**. The vaults at the far right have the highest values, and you don't lose anything by skipping the smaller ones on the left.

However, when you pick a vault, you can‚Äôt pick any other vault within `k` positions on either side. So after picking one vault, you skip `k` vaults to the left and keep repeating that until you run out of vaults.  

Basically, you're dividing the vaults into groups of size `k + 1`, and you always take the **rightmost vault** from each group because that's the one with the highest value in that group.

**The algorithm picks:**
n, n-(k+1), n-2(k+1), ‚Ä¶
until you reach the beginning.  
It‚Äôs a simple one-pass process, which is why it runs in **Œò(n)** time.

---

### ‚úÖ Correctness Proof

In this problem, all the vault values go up or stay the same as we move to the right, meaning every vault on the right side is at least as valuable as the ones before it. Because of that, picking vaults farther to the right will never give a smaller total value.

Each time we pick a vault, we're not allowed to pick any other vault within `k` positions on either side. This means that one vault **‚Äúuses up‚Äù** a total of `k + 1` spots (itself and its `k` neighbors). So, at most, we can pick around one vault for every `k + 1` positions.  

If we think of the vaults as being split into groups of `k + 1`, then every valid solution can take at most one vault per group. Now, because the values never decrease, the **rightmost vault** in each group will always be the most valuable one.  

The greedy algorithm takes advantage of this by starting from the end of the list and always picking that rightmost vault, then skipping `k` positions to the left and repeating.  

To see why this is always optimal, imagine there was another solution that gave a higher total value. Look at the first group (from the right) where that solution did **not** choose the rightmost value. If we move their chosen vault to the rightmost position of that group, two things happen:

1. ‚úÖ The spacing rule is still valid because moving a vault to the right only increases the distance from previous picks.  
2. üí∞ The total value doesn‚Äôt go down since the rightmost vault‚Äôs value is greater than or equal to the one it replaced.  

If we keep applying this idea to every group, we can gradually transform any other valid solution into the greedy one without lowering its total value.  

This means the greedy solution must already be optimal.  
Therefore, **Algorithm 1 always finds the best possible total value for Problem S1.**

---

### üïí Time Complexity
**Œò(n)** ‚Äì the algorithm only passes through the vaults once.

---

### üß† Summary
- **Type:** Greedy algorithm  
- **Special Case:** Monotonically non-decreasing vault values  
- **Key Idea:** Always pick the rightmost available vault  
- **Why It Works:** Later vaults are never worse than earlier ones, so picking right-to-left guarantees the best result.

Mya - Currently Working on Algorithm 2 Problem S2 
---

## üß© Problem S2 ‚Äì Unimodal Vault Values (Single Local Minimum)

### üí° Dynamic Programming Idea

In this problem, the vault values first **decrease** and then **increase**, forming a **unimodal (V-shaped)** sequence with a single local minimum.  
Because the sequence is not strictly increasing or decreasing, the greedy approach from Problem S1 no longer guarantees the optimal result.

To maximize the total vault value while ensuring that no two chosen vaults are within `k` positions of each other, we use a **dynamic programming (DP)** approach.

At each vault `i`, we have two options:

- **Option 1 ‚Äì Skip it:**  
  The best total remains `dp[i-1]`.

- **Option 2 ‚Äì Take it:**  
  We add `values[i]` to `dp[i-(k+1)]` since we must skip the previous `k` vaults.

We then choose the option that yields the higher total:

\[
dp[i] = \max(dp[i-1],\; values[i] + dp[i-(k+1)])
\]

The algorithm computes these values iteratively while tracking which vaults were taken.  
At the end, we reconstruct the optimal list of chosen vaults (1-indexed) from the recorded decisions.

---

### ‚úÖ Correctness Proof

Let `OPT(i)` represent the maximum total value achievable using the first `i` vaults under the spacing constraint.  
We can express this recursively as:

- If we **skip** vault `i`,  
  then `OPT(i) = OPT(i-1)`.
- If we **take** vault `i`,  
  then `OPT(i) = values[i] + OPT(i-(k+1))`.

Each subproblem depends only on smaller subproblems that follow the same structure, satisfying the **optimal substructure** property.  
Because we always choose the maximum of the two possible outcomes for every state, dynamic programming ensures that the overall result is globally optimal.

By backtracking through the stored decisions, we reconstruct a valid sequence of indices that achieves this maximum total.  
Therefore, the DP algorithm always produces an **optimal** solution.

---

### üïí Time Complexity
**Œò(n)** ‚Äì The algorithm only makes a single pass through all `n` vaults, performing constant-time work per iteration.

---

### üß† Summary
- **Type:** Dynamic Programming  
- **Special Case:** Unimodal (V-shaped) vault values  
- **Key Idea:** Evaluate both "take" and "skip" options for each vault  
- **Why It Works:**  
  The optimal solution can be built from the best solutions to smaller subproblems, guaranteeing maximum total value while respecting the `k`-spacing constraint.
